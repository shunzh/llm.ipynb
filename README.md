# ðŸ§  Deep Learning from Scratch

Minimal PyTorch implementations of core deep learning components, built from scratch to understand how they work.

## ðŸ““ Notebooks

- [`llm.ipynb`](./llm.ipynb)  
  A minimal decoder-only Transformer for language modeling, with code for architecture, training, and generation.

- [`adamw.ipynb`](./adamw.ipynb)  
  A from-scratch implementation of the AdamW optimizer, comparing it against L2-regularized Adam on a toy regression task.

ðŸ’¡ **Tip:** Run these notebooks directly in Google Colab â€” no setup required.
