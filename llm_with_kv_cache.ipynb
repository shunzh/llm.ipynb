{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunzh/llm.ipynb/blob/main/llm_with_kv_cache.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbgcy_Lcnb6K"
      },
      "source": [
        "# ðŸ“˜ Implementing a Decoder-Only Transformer from Scratch (with KV Cache)\n",
        "\n",
        "We implement a decoder-only Transformer with kv cache to improve its inference efficiency.\n",
        "\n",
        "We recommend going through the [original implementation without KV cache](https://github.com/shunzh/llm.ipynb/blob/main/llm.ipynb) first.\n",
        "\n",
        "We first install and import the necessary libraries and set the random seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XrQMXHWj4nKj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Set random seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ2VP8xCpMYw"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "We first define a model config object with default parameter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EAIQppvnMOeZ"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # The size of hidden state in transformer, also called d_model\n",
        "    hidden_size: int = 512\n",
        "    # The size of hidden state in MLP in the decoder block\n",
        "    ff_hidden_size: int = 4 * 512\n",
        "    # The number of decoder blocks\n",
        "    num_hidden_layers: int = 2\n",
        "    # Dropout rates for all modules that need dropout\n",
        "    dropout_rate: float = 0.1\n",
        "    vocab_size: int = 10000\n",
        "    max_seq_len: int = 128\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6gyx0Q9Llde"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "The decoder block is the core of the transformer. It has four modules inside (where the LayerNorm appears twice):\n",
        "\n",
        "```\n",
        "Input: x (batch_size, seq_len, hidden_size)\n",
        "        â”‚\n",
        "        â–¼\n",
        "+-------------------+\n",
        "|   LayerNorm       |\n",
        "+-------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+-------------------+\n",
        "|  Self-Attention   |  (single head, with causal mask)\n",
        "+-------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+-------------------+\n",
        "|   LayerNorm       |\n",
        "+-------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+-------------------+\n",
        "|      MLP          |  (Linear â†’ GELU â†’ Linear)\n",
        "+-------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "     Output x (batch_size, seq_len, hidden_size)\n",
        "```\n",
        "\n",
        "Let's define these modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xc5qVYV3Nz-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2780f9b-9311-4971-fb90-6b19a3887103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of layer norm output is (batch_size, seq_len, hidden_size): torch.Size([1, 3, 512])\n"
          ]
        }
      ],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size)) # (hidden_size,)\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size)) # (hidden_size,)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, hidden_size)\n",
        "        mean = x.mean(dim=-1, keepdim=True) # (batch_size, seq_len, 1)\n",
        "        std = x.std(dim=-1, keepdim=True) # (batch_size, seq_len, 1)\n",
        "        return (x - mean) / (std + self.eps) * self.gamma + self.beta # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "\n",
        "# Test layer norm\n",
        "layer_norm = LayerNorm(config.hidden_size)\n",
        "x = torch.randn(1, 3, config.hidden_size)\n",
        "layer_norm_output = layer_norm(x)\n",
        "print(\"The size of layer norm output is (batch_size, seq_len, hidden_size):\", layer_norm_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E6LJFxvYtD7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020a28d1-e6f2-49da-b782-ad6528135114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output without cache:\n",
            "tensor([[[-0.6716, -0.3282, -0.6029,  ...,  0.0846, -0.9988, -0.0512],\n",
            "         [-0.6365, -0.0690, -0.4434,  ...,  0.1903, -0.4347,  0.0625],\n",
            "         [-0.2435,  0.2562, -0.2738,  ...,  0.1553, -0.2531, -0.0460]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Output with cache:\n",
            "tensor([[[-0.6716, -0.3282, -0.6029,  ...,  0.0846, -0.9988, -0.0512],\n",
            "         [-0.6365, -0.0690, -0.4434,  ...,  0.1903, -0.4347,  0.0625],\n",
            "         [-0.2435,  0.2562, -0.2738,  ...,  0.1553, -0.2531, -0.0460]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "âœ… Outputs match with and without KV cache\n"
          ]
        }
      ],
      "source": [
        "class SingleHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Attention: hidden state to query, key, value\n",
        "        self.c_attn = nn.Linear(hidden_size, 3 * hidden_size)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Dropouts\n",
        "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
        "        self.proj_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, past_kv=None, use_cache=False):\n",
        "        # x: (batch_size, seq_len, hidden_size)\n",
        "        # If use_cache == True, x only contain new tokens because previous tokens are not needed,\n",
        "        #   past_kv is a tuple of (past_k, past_v), each is (batch_size, past_len, hidden_size),\n",
        "        #   where past_len is the length of cached tokens\n",
        "        # let total_len = past_len + seq_len if use_cache else seq_len\n",
        "        batch_size, seq_len, hidden_size_in_data = x.shape\n",
        "        assert self.hidden_size == hidden_size_in_data, f\"Mismatch between hidden_size in config {self.hidden_size} and hidden_size in data {hidden_size_in_data}\"\n",
        "\n",
        "        c_attn_output = self.c_attn(x) # (batch_size, seq_len, 3 * hidden_size)\n",
        "\n",
        "        # Split into query, key, and value\n",
        "        q, k, v = c_attn_output.split(self.hidden_size, dim=-1)\n",
        "\n",
        "        if past_kv is not None:\n",
        "            past_k, past_v = past_kv\n",
        "            k = torch.cat([past_k, k], dim=1) # (batch_size, total_len, hidden_size)\n",
        "            v = torch.cat([past_v, v], dim=1) # (batch_size, total_len, hidden_size)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # q: (.., seq_len, hidden_size)\n",
        "        # k.transpose(-2, -1): (.., hidden_size, total_len)\n",
        "        attn = (q @ k.transpose(-2, -1)) / (math.sqrt(self.hidden_size)) # (batch_size, seq_len, total_len)\n",
        "\n",
        "        # Apply causal mask\n",
        "        total_len = attn.shape[-1]\n",
        "        # If use_cache = True, seq_len = 2, the mask looks like\n",
        "        # [[1, 1, .., 1, 0]]\n",
        "        # [[1, 1, .., 1, 1]]\n",
        "        # (although seq_len = 1 during inference)\n",
        "        mask = torch.tril(torch.ones(1, total_len, total_len))[:, -seq_len:, :].to(attn.device) # (1, seq_len, total_len)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # Softmax\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "\n",
        "        # Dropout attention\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        # Attention output\n",
        "        # attn: (.., seq_len, seq_len)\n",
        "        # v: (.., seq_len, d_head)\n",
        "        attn_output = attn @ v # (batch_size, seq_len, d_head)\n",
        "\n",
        "        # Final projection\n",
        "        proj_output = self.c_proj(attn_output)\n",
        "        proj_output = self.proj_dropout(proj_output) # (batch_size, seq_len, d_head)\n",
        "\n",
        "        if use_cache:\n",
        "            new_kv = (k, v) # Each (batch_size, total_len, hidden_size)\n",
        "            return proj_output, new_kv\n",
        "        else:\n",
        "            return proj_output\n",
        "\n",
        "\n",
        "# Test with and without kv cache\n",
        "# Assume config.hidden_size is already set\n",
        "x = torch.randn(1, 3, config.hidden_size)\n",
        "self_attention = SingleHeadSelfAttention(config.hidden_size)\n",
        "self_attention.eval()  # disable dropout\n",
        "\n",
        "# Forward without cache\n",
        "out_no_cache = self_attention(x, use_cache=False)\n",
        "\n",
        "# Forward with cache (step by step)\n",
        "past_kv = None\n",
        "outputs = []\n",
        "for i in range(x.size(1)):\n",
        "    token = x[:, i:i+1]\n",
        "    out, past_kv = self_attention(token, past_kv=past_kv, use_cache=True)\n",
        "    outputs.append(out)\n",
        "out_with_cache = torch.cat(outputs, dim=1)\n",
        "\n",
        "# Print results\n",
        "print(\"Output without cache:\")\n",
        "print(out_no_cache)\n",
        "\n",
        "print(\"\\nOutput with cache:\")\n",
        "print(out_with_cache)\n",
        "\n",
        "assert torch.allclose(out_no_cache, out_with_cache, atol=1e-5), \"Outputs do not match with and without KV cache\"\n",
        "print(\"âœ… Outputs match with and without KV cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "psUlysBkKo94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b37f1b-e671-43e4-beaa-405d1b34f115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 3, 512])\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size, ff_hidden_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.ff_hidden_size = ff_hidden_size\n",
        "\n",
        "        self.c_fc = nn.Linear(self.hidden_size, self.ff_hidden_size)\n",
        "        self.act = nn.GELU() # Or other activation functions\n",
        "        self.c_proj = nn.Linear(self.ff_hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, hidden_size)\n",
        "        x = self.c_fc(x) # (batch_size, seq_len, ff_hidden_size)\n",
        "        x = self.act(x) # (batch_size, seq_len, ff_hidden_size)\n",
        "        x = self.c_proj(x) # (batch_size, seq_len, hidden_size)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test MLP\n",
        "x = torch.randn(1, 3, config.hidden_size)\n",
        "mlp = MLP(config.hidden_size, config.ff_hidden_size)\n",
        "output = mlp(x)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8yBkjCLNMlj"
      },
      "source": [
        "With all the pieces defined above, we're ready to define the decoder block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8tGDbfH6NQID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c879d96-5d64-436d-ecab-23b8d43f348f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output without cache:\n",
            " tensor([[[ 0.1712,  0.2079, -0.2557,  ...,  0.4331, -0.7992,  0.5643],\n",
            "         [-1.7891, -1.1413,  1.2852,  ..., -0.1187,  1.4198,  0.8264],\n",
            "         [-0.7370, -0.2091,  1.4891,  ...,  0.2402,  1.4331, -0.9390]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "Output with cache:\n",
            " tensor([[[ 0.1712,  0.2079, -0.2557,  ...,  0.4331, -0.7992,  0.5643],\n",
            "         [-1.7891, -1.1413,  1.2852,  ..., -0.1187,  1.4198,  0.8264],\n",
            "         [-0.7370, -0.2091,  1.4891,  ...,  0.2402,  1.4331, -0.9390]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "âœ… Outputs match with and without KV cache\n"
          ]
        }
      ],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, ff_hidden_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(hidden_size)\n",
        "        self.attn = SingleHeadSelfAttention(hidden_size, dropout_rate)\n",
        "        self.ln_2 = LayerNorm(hidden_size)\n",
        "        self.mlp = MLP(hidden_size, ff_hidden_size, dropout_rate)\n",
        "\n",
        "    def forward(self, x, past_kv=None, use_cache=False):\n",
        "        # x: (batch_size, seq_len, hidden_size)\n",
        "        # Layer norm 1\n",
        "        x = self.ln_1(x)\n",
        "        # Self attention + residual\n",
        "        if use_cache:\n",
        "            attn_output, new_kv = self.attn(x, past_kv=past_kv, use_cache=True)\n",
        "            x = x + attn_output\n",
        "        else:\n",
        "            attn_output = self.attn(x)\n",
        "            x = x + attn_output\n",
        "        # Layer norm 2\n",
        "        x = self.ln_2(x)\n",
        "        # MLP + residual\n",
        "        x = x + self.mlp(x)\n",
        "\n",
        "        if use_cache:\n",
        "            return x, new_kv\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "# Test Decoder\n",
        "x = torch.randn(1, 3, config.hidden_size)\n",
        "decoder = DecoderBlock(config.hidden_size, config.ff_hidden_size)\n",
        "decoder.eval()  # disable dropout\n",
        "\n",
        "# Forward without cache\n",
        "out_full = decoder(x, use_cache=False)\n",
        "\n",
        "# Forward with cache (incremental)\n",
        "past_kv = None\n",
        "outputs = []\n",
        "for i in range(x.size(1)):\n",
        "    token = x[:, i:i+1]\n",
        "    out_step, past_kv = decoder(token, past_kv=past_kv, use_cache=True)\n",
        "    outputs.append(out_step)\n",
        "out_cached = torch.cat(outputs, dim=1)\n",
        "\n",
        "# Compare\n",
        "print(\"Output without cache:\\n\", out_full)\n",
        "print(\"\\nOutput with cache:\\n\", out_cached)\n",
        "\n",
        "assert torch.allclose(out_full, out_cached, atol=1e-5), \"Outputs do not match with and without KV cache\"\n",
        "print(\"âœ… Outputs match with and without KV cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIT2gSUtf3M_"
      },
      "source": [
        "### The Complete Transformer Model\n",
        "\n",
        "With the Decoder block defined above, we're ready to define the complete Transformer architecture.\n",
        "\n",
        "```\n",
        "Input: input_ids (batch_size, seq_len)\n",
        "        â”‚\n",
        "        â–¼\n",
        "+------------------------+\n",
        "|  Token Embedding       |\n",
        "|  Position Embedding    |\n",
        "+------------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+------------------------+\n",
        "|   DecoderBlock Ã— N     |  (defined in the previous section)\n",
        "+------------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+------------------------+\n",
        "|   Final LayerNorm      |\n",
        "+------------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "+------------------------+\n",
        "|  Linear (Language Head)|\n",
        "+------------------------+\n",
        "        â”‚\n",
        "        â–¼\n",
        "Output: logits (batch_size, seq_len, vocab_size)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lHXI6NE32YBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211c7926-a66f-4cf5-c60b-8c95c80a9685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Transformer model:\n",
            " Transformer(\n",
            "  (token_embed): Embedding(10000, 512)\n",
            "  (position_embed): Embedding(128, 512)\n",
            "  (embed_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0-1): 2 x DecoderBlock(\n",
            "      (ln_1): LayerNorm()\n",
            "      (attn): SingleHeadSelfAttention(\n",
            "        (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm()\n",
            "      (mlp): MLP(\n",
            "        (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (language_head): Linear(in_features=512, out_features=10000, bias=True)\n",
            ")\n",
            "Full logits:\n",
            " tensor([[[-0.1600,  0.5028,  0.7586,  ...,  0.1134, -0.8263,  1.2630],\n",
            "         [-0.1174,  0.1616,  0.0864,  ..., -0.8400, -0.8155, -0.2362],\n",
            "         [ 0.1726,  0.4938,  0.2812,  ...,  0.4364,  0.1020, -0.1239]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "Cached logits:\n",
            " tensor([[[-0.1600,  0.5028,  0.7586,  ...,  0.1134, -0.8263,  1.2630],\n",
            "         [-0.1174,  0.1616,  0.0864,  ..., -0.8400, -0.8155, -0.2362],\n",
            "         [ 0.1726,  0.4938,  0.2812,  ...,  0.4364,  0.1020, -0.1239]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "âœ… Outputs match with and without KV cache\n"
          ]
        }
      ],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size,\n",
        "        ff_hidden_size,\n",
        "        vocab_size,\n",
        "        max_seq_len,\n",
        "        num_hidden_layers,\n",
        "        dropout_rate=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.position_embed = nn.Embedding(max_seq_len, hidden_size)\n",
        "        self.embed_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([DecoderBlock(hidden_size, ff_hidden_size, dropout_rate) for _ in range(num_hidden_layers)])\n",
        "        # The final layer norm\n",
        "        self.ln_f = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # The final language head, which maps the last hidden state to logits\n",
        "        self.language_head = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        past_kvs=None,\n",
        "        use_cache=False,\n",
        "    ):\n",
        "        # input_ids: (batch_size, seq_len)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        if past_kvs:\n",
        "            past_len = past_kvs[0][0].shape[1]\n",
        "        else:\n",
        "            past_len = 0\n",
        "\n",
        "        # Create position ids (past_len, past_len + 1, ..)\n",
        "        position_ids = torch.arange(past_len, past_len + seq_len, device=input_ids.device).unsqueeze(0)  # (1, seq_len)\n",
        "\n",
        "        # Embed tokens and positions, apply dropout\n",
        "        x = self.token_embed(input_ids) + self.position_embed(position_ids)\n",
        "        x = self.embed_dropout(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        new_kvs = [] if use_cache else None # new_kvs[i] will be the new_kv for the i-th layer\n",
        "        for i, layer in enumerate(self.hidden_layers):\n",
        "            past_kv = past_kvs[i] if past_kvs is not None else None\n",
        "            if use_cache:\n",
        "                x, new_kv = layer(x, past_kv=past_kv, use_cache=True)\n",
        "                new_kvs.append(new_kv)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        # Final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.language_head(x) # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        if use_cache:\n",
        "            return logits, new_kvs\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "# Test Transformer\n",
        "input_ids = torch.randint(0, config.vocab_size, (1, 3))\n",
        "\n",
        "model = Transformer(\n",
        "    hidden_size=config.hidden_size,\n",
        "    ff_hidden_size=config.ff_hidden_size,\n",
        "    vocab_size=config.vocab_size,\n",
        "    max_seq_len=config.max_seq_len,\n",
        "    num_hidden_layers=config.num_hidden_layers,\n",
        "    dropout_rate=config.dropout_rate,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Our Transformer model:\\n\", model)\n",
        "\n",
        "# Forward without cache\n",
        "logits_full = model(input_ids, use_cache=False)\n",
        "\n",
        "# Forward with cache (token-by-token)\n",
        "past_kvs = None\n",
        "logits_cached = []\n",
        "for i in range(input_ids.size(1)):\n",
        "    logits_step, past_kvs = model(input_ids[:, i:i+1], past_kvs=past_kvs, use_cache=True)\n",
        "    logits_cached.append(logits_step)\n",
        "logits_cached = torch.cat(logits_cached, dim=1)\n",
        "\n",
        "# Compare\n",
        "print(\"Full logits:\\n\", logits_full)\n",
        "print(\"Cached logits:\\n\", logits_cached)\n",
        "\n",
        "assert torch.allclose(logits_full, logits_cached, atol=1e-5), \"Outputs do not match with and without KV cache\"\n",
        "print(\"âœ… Outputs match with and without KV cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM3gL5p6PRfy"
      },
      "source": [
        "## Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V-LNpgjDBOL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Update config\n",
        "config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Tokenize entire corpus\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set a path to save the trained checkpoint so it can be retrieved later for inference."
      ],
      "metadata": {
        "id": "Nev3XgQ6GqYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set model path here\n",
        "model_checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/model.pth\"\n",
        "\n",
        "# Or save the checkpoint to this runtime without connecting to Google Drive.\n",
        "# The checkpoint will be deleted after the runtime terminates.\n",
        "# model_checkpoint_path = \"/content/model.pth\""
      ],
      "metadata": {
        "id": "UgLF1mD9qchS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32de7c8-acd8-4f69-a740-4b488bb028c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdpJYH_TZsdJ"
      },
      "source": [
        "### Model Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first load the trained model from the previous cell."
      ],
      "metadata": {
        "id": "8uqr-jBVNGkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  The training cell above may have been killed. So redefine these variables here.\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Using device\", device)\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    hidden_size=config.hidden_size,\n",
        "    ff_hidden_size=config.ff_hidden_size,\n",
        "    vocab_size=config.vocab_size,\n",
        "    max_seq_len=config.max_seq_len,\n",
        "    num_hidden_layers=config.num_hidden_layers,\n",
        "    dropout_rate=config.dropout_rate,\n",
        ").to(device)\n",
        "\n",
        "# Load trained model\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_checkpoint_path, map_location=device))\n",
        "except:\n",
        "    print(f\"Checkpoint {model_checkpoint_path} not found. Skip loading.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgPQ0oe0NBlM",
        "outputId": "c3df5bdb-0c4d-4ad8-88aa-6542e46d3bfb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Greedy Decoding\n",
        "\n",
        "Since we're using a very small model and a very limited training set, the generated outputs may not be semantically meaningful.\n",
        "Still, let's run greedy decoding with the trained model and see what it produces!"
      ],
      "metadata": {
        "id": "WtDWu2nwNATm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkk_gYlHfSu5",
        "outputId": "062bac21-8d09-40ac-cbc7-46454494a854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Output with NO KV Cache ====\n",
            ",\n",
            "And, as I am a king,\n",
            "And, as I do not, and I am sure,\n",
            "And, as I am a king,\n",
            "And yet I am not to be a king.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I am a gentleman, and I am a king,\n",
            "And so I am a king, and a\n",
            "motion generative to the crown.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know you, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir\n",
            "==== End of Outpute ====\n",
            "Time taken: 1.1075119972229004 seconds\n",
            "\n",
            "==== Output with KV Cache ====\n",
            ",\n",
            "And, as I am a king,\n",
            "And, as I do not, and I am sure,\n",
            "And, as I am a king,\n",
            "And yet I am not to be a king.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I am a gentleman, and I am a king,\n",
            "And so I am a king, and a\n",
            "motion generative to the crown.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know you, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir, sir\n",
            "==== End of Outpute ====\n",
            "Time taken: 0.3300652503967285 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def greedy_decode(model, input_ids, max_len, device=torch.device(\"cpu\")):\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_len - input_ids.size(1)):\n",
        "        logits = model(input_ids) # (batch_size, seq_len, vocab_size)\n",
        "        next_tokens = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) # (batch_size, 1)\n",
        "        input_ids = torch.cat((input_ids, next_tokens), dim=-1)\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def greedy_decode_with_kv_cache(model, input_ids, max_len, device=torch.device(\"cpu\")):\n",
        "    model.eval()\n",
        "\n",
        "    past_kvs = None\n",
        "    generated = input_ids\n",
        "\n",
        "    for _ in range(max_len - input_ids.size(1)):\n",
        "        logits, past_kvs = model(input_ids, past_kvs=past_kvs, use_cache=True)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        generated = torch.cat([generated, next_token], dim=-1)\n",
        "        input_ids = next_token  # only feed the new token next step\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "# Use an empty prompt\n",
        "input_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(device)\n",
        "\n",
        "# Run greedy decoding\n",
        "start_time = time.time()\n",
        "output_ids = greedy_decode(model, input_ids, max_len=128, device=device)\n",
        "end_time = time.time()\n",
        "generated_text = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "print(\"==== Output with NO KV Cache ====\")\n",
        "print(generated_text)\n",
        "print(\"==== End of Outpute ====\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")\n",
        "print()\n",
        "\n",
        "# Run greedy decoding with kv cache\n",
        "start_time = time.time()\n",
        "output_ids = greedy_decode_with_kv_cache(model, input_ids, max_len=128, device=device)\n",
        "end_time = time.time()\n",
        "generated_text = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "print(\"==== Output with KV Cache ====\")\n",
        "print(generated_text)\n",
        "print(\"==== End of Outpute ====\")\n",
        "print(f\"Time taken: {end_time - start_time} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}